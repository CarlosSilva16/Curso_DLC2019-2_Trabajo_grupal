{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING\n",
    "\n",
    "## Objetivo:\n",
    "\n",
    "Aplique los temas vistos en los seminarios de Deep Learning en sus proyectos de investigación y familiarícese con librerías como word2vec y keras.\n",
    "\n",
    "## Trabajo Grupal\n",
    "### Integrantes\n",
    "* Carlos Alfredo Silva Villafuerte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumen de los datos usados:\n",
    "5917 publicaciones \n",
    "\n",
    "pip install Cython\n",
    "pip install word2vec unidecode nltk\tstop_words sklearn scipy matplotlib\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Training the word embeddings model, word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use of some steps like tokenize, conversion of characters to lower-case, conversion of special characters á -> a, ñ->n\n",
    "## removal of punctuation and removal of stopwords (the, to, an, and, a, this)\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'word2vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9bc290847df0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'word2vec'"
     ]
    }
   ],
   "source": [
    "import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_datapath = \"Publicaciones.csv\"\n",
    "file_datapathTxt = \"Publicaciones.txt\"\n",
    "file_datapathTxtWithoutNoise = \"Publicaciones_without_Noise.txt\"\n",
    "df = pd.read_csv(file_datapath, sep=',', comment='#' , error_bad_lines=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "stop_words = [unidecode(stopW) for stopW in stopwords.words('spanish')]\n",
    "\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡', 'q', 'd', 'x', 'xq', '...', '..']) #stopwords that are not present in the official punctuation/stopwords lists, then we added them \n",
    "stop_words = stop_words + non_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For you to try. Make a list of sentences that would need to be preprocessed\n",
    "dirty_sentences = np.array(df['publicacion'])\n",
    "#Clean yor sentences and save them in another list\n",
    "clean_sentences=[] #list to put previous sentences once they are preprocessed\n",
    "for dirty_sent in dirty_sentences:\n",
    "    dirty_sent = str(dirty_sent)\n",
    "    tokens=[token.lower() for token in tknzr.tokenize(unidecode(dirty_sent)) \n",
    "                          if token.lower() not in stop_words]\n",
    "    processed_sentence = ' '.join(tokens) \n",
    "    clean_sentences.append(processed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Create a file only with the publications titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publicacion'].to_csv(file_datapathTxt, header=True, index=False, sep='\\t', mode='a')\n",
    "df_clean_sentences = pd.DataFrame(clean_sentences)\n",
    "df_clean_sentences.to_csv(file_datapathTxtWithoutNoise, header=True, index=False, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model giving as input the training dataset (preprocesssed tweets in txt file) and the name of the file where we are going to save the model\n",
    "#in the parameters we have assigned values for size that represents the number of dimensions for the result vectors and window to consider in which 'context' the evaluated word is going to be (words before and after target)\n",
    "word2vec.word2vec(file_datapathTxtWithoutNoise, 'Publicaciones_model.bin', size=300, window= 5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load('Publicaciones_model.bin') #load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the work of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.vectors.shape)   #verify the size of the vocabulary (number of words from which we extracted the vectors) and the dimensions of vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes, metrics = model.similar('universidad',10)  #target word and the number of desired results (similar words)\n",
    "model.generate_response(indexes, metrics).tolist() #sentence to print results: word, measure_of_similarity (being 1 the most similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocab[10:20]  #model.vocab contains the list of words in the vocabulary model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['</s>']   #if we call the model specifying a given word, it is going to present the 300dim vector that belongs to that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vectors[0] #this is another way to get the vector of a word if we know in which position in model.vocab the word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next you have some necessary steps part of the transformation of 300dim to 2D in order to plot\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can think of a group of words that make sense to be together/far away\n",
    "toplot_words=['universidad','fincas', 'escuelas', \n",
    "            'harina','vacteria',\n",
    "            'sistema','eficiencia','estabilidad','evaluacion' \n",
    "           ]\n",
    "\n",
    "toplot_vecs=[model[w] for w in toplot_words] #getting the vectors for the specific words in toplot_words\n",
    "result = pca.fit_transform(toplot_vecs) #call to pca for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally plotting...\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(toplot_words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
