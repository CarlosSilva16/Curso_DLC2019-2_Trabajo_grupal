{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING\n",
    "\n",
    "## Objetivo:\n",
    "\n",
    "Aplique los temas vistos en los seminarios de Deep Learning en sus proyectos de investigación y familiarícese con librerías como word2vec y keras.\n",
    "\n",
    "## Trabajo Grupal\n",
    "### Integrantes\n",
    "* Carlos Alfredo Silva Villafuerte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pip install Cython\n",
    "* pip install word2vec\n",
    "* pip\tinstall\tunidecode\t\n",
    "* pip\tinstall\tnltk\t\n",
    "* pip\tinstall\tstop_words\t\n",
    "* pip\tinstall\tsklearn\t\n",
    "* pip\tinstall\tscipy\t\n",
    "* pip\tinstall\tmatplotlib\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from unidecode import unidecode\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_datapath = \"Publicaciones.csv\"\n",
    "file_datapathTxt = \"Publicaciones.txt\"\n",
    "file_datapathTxtWithoutNoise = \"Publicaciones_without_Noise.txt\"\n",
    "df = pd.read_csv(file_datapath, sep=',', comment='#' , error_bad_lines=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()\n",
    "stop_words = [unidecode(stopW) for stopW in stopwords.words('spanish')]\n",
    "\n",
    "# add aditional stop words\n",
    "non_words = list(punctuation)\n",
    "non_words.extend(['¿', '¡', 'q', 'd', 'x', 'xq', '...', '..'])\n",
    "stop_words = stop_words + non_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recorer cada publicacion\n",
    "dirty_sentences = np.array(df['publicacion'])\n",
    "#dirty_sentences = dirty_sentences[np.logical_not(np.isnan(dirty_sentences))]\n",
    "\n",
    "#Clean yor sentences and save them in another list\n",
    "clean_sentences=[] #list to put previous sentences once they are preprocessed\n",
    "for dirty_sent in dirty_sentences:\n",
    "    #dirty_sent = str(dirty_sent).encode('utf-8')\n",
    "    #print(dirty_sent)\n",
    "    tokens=[token.lower() for token in tknzr.tokenize(unidecode(dirty_sent)) \n",
    "                          if token.lower() not in stop_words]\n",
    "    processed_sentence = ' '.join(tokens) \n",
    "    clean_sentences.append(processed_sentence)\n",
    "np.savetxt(file_datapathTxtWithoutNoise, clean_sentences, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1) Create a file only with the publications titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publicacion'].to_csv(file_datapathTxt, header=True, index=False, sep='\\t', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2vec \n",
    "word2vec.word2vec(file_datapathTxt, 'Publicaciones_model.bin', size=300, window= 5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.load('Publicaciones_model.bin') #load the trained model\n",
    "print(model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes, metrics = model.similar('estudio',10)  \n",
    "model.generate_response(indexes, metrics).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.vocab[10:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
